{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ce12f59",
   "metadata": {},
   "source": [
    "# LinkedIn Scraper\n",
    "Este notebook coleta dados de perfis do LinkedIn que interagiram com postagens relacionadas à pesquisa \"State of the Data 2024\". O objetivo é extrair informações desses perfis e tentar reidentificá-los no dataset público divulgado pela pesquisa.\n",
    "\n",
    "\n",
    "Coletamos dados de posts em que a pesquisa estava sendo divulgada para preenchimento. Dentre os posts coletados estão os seguintes:\n",
    "\n",
    "https://www.linkedin.com/posts/crbazevedo_state-of-data-brazil-2024-activity-7272031218773225472-k2du/\n",
    "\n",
    "https://www.linkedin.com/posts/paulovasconcellos_a-pesquisa-state-of-data-2425-já-está-no-activity-7252287169589878785-3tyi\n",
    "\n",
    "https://www.linkedin.com/posts/data-hackers_data-dados-empresas-activity-6990305987286962176-O05v\n",
    "\n",
    "Se fizer sentido coletar dados de mais pessoas, o seguinte post pode ser uma boa:\n",
    "\n",
    "https://www.linkedin.com/posts/gabrielclages_um-pedido-rápido-e-direto-ao-ponto-se-você-activity-7262525821897113600-A7zt\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48186b89",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637eb85a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from linkedin_scraper import actions\n",
    "from linkedin_scraper import Person, Company\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from typing import Dict, List, Optional, Union\n",
    "import time\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import openai\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95a054b2",
   "metadata": {},
   "source": [
    "## Autenticação e Navegação\n",
    "\n",
    "Funções auxiliares e abertura da página. O login no linkedin deve ser feito manualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a2625a",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def setup_driver():\n",
    "    return webdriver.Chrome()\n",
    "\n",
    "def login(driver, email, password):\n",
    "    actions.login(driver, email, password)\n",
    "\n",
    "def open_page(driver, url):\n",
    "    driver.get(url)\n",
    "    time.sleep(3)\n",
    "\n",
    "def save_to_csv(profiles, filename=\"linkedin_profiles.csv\"):\n",
    "    df = pd.DataFrame(profiles)\n",
    "    file_exists = os.path.isfile(filename)\n",
    "\n",
    "    df.to_csv(filename, mode='a', header=not file_exists, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Adicionados {len(profiles)} perfis ao arquivo {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e90698cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = setup_driver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052a7f2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "wait = WebDriverWait(driver, 10)\n",
    "\n",
    "url = \"https://www.linkedin.com/posts/gabrielclages_um-pedido-rápido-e-direto-ao-ponto-se-você-activity-7262525821897113600-A7zt\"\n",
    "\n",
    "open_page(driver, url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561de7b3",
   "metadata": {},
   "source": [
    "## Coleta de perfis que interagiram\n",
    "As duas funções abaixo fazem o seguinte:\n",
    "\n",
    " - Abrem o pupup com todas as pessoas que interagiram com o post\n",
    " - Rola a lista até o fim para carregar todas as pessoas\n",
    " - Coleta nome, link do perfil e descrição de cada um\n",
    " - Salva o resultado em 'linkedin_profiles.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436e3905",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def scroll_to_bottom_and_click_more(driver, wait, scrollable):\n",
    "    last_height = driver.execute_script(\"return arguments[0].scrollHeight\", scrollable)\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(\"arguments[0].scrollTo(0, arguments[0].scrollHeight);\", scrollable)\n",
    "        time.sleep(1)\n",
    "\n",
    "        try:\n",
    "            show_more = wait.until(EC.element_to_be_clickable(\n",
    "                (By.CSS_SELECTOR, \"button.scaffold-finite-scroll__load-button\")\n",
    "            ))\n",
    "            # show_more.click()\n",
    "            time.sleep(2)\n",
    "        except:\n",
    "            print(\"Botão 'Show more results' não encontrado ou não clicável.\")\n",
    "            break\n",
    "\n",
    "        new_height = driver.execute_script(\"return arguments[0].scrollHeight\", scrollable)\n",
    "        if new_height == last_height:\n",
    "            print(\"Nenhum conteúdo novo foi carregado.\")\n",
    "            break\n",
    "        last_height = new_height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548c5909",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extract_profiles(driver):\n",
    "    profiles = []\n",
    "    profile_blocks = driver.find_elements(By.CSS_SELECTOR, \"a.link-without-hover-state\")\n",
    "\n",
    "    for i, block in enumerate(profile_blocks):\n",
    "        try:\n",
    "            link = block.get_attribute(\"href\")\n",
    "            name_elem = block.find_element(By.CSS_SELECTOR, \".artdeco-entity-lockup__title span.text-view-model\")\n",
    "            name = name_elem.text.strip()\n",
    "\n",
    "            desc_elem = block.find_element(By.CSS_SELECTOR, \".artdeco-entity-lockup__caption\")\n",
    "            description = desc_elem.text.strip()\n",
    "\n",
    "            profiles.append({\n",
    "                \"Nome\": name,\n",
    "                \"Link\": link,\n",
    "                \"Descrição\": description\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"[{i}] Erro ao processar: {e}\")\n",
    "    \n",
    "    print(f\"Total de perfis extraídos: {len(profiles)}\")\n",
    "    \n",
    "    return profiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4b939",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "elements = driver.find_elements(By.CSS_SELECTOR, \".social-details-social-counts__reactions-count\")\n",
    "for i, element in enumerate(elements):\n",
    "    try:\n",
    "        element.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "        scrollable = driver.find_element(By.CSS_SELECTOR, \"div.artdeco-modal__content\")\n",
    "        scroll_to_bottom_and_click_more(driver, wait, scrollable)\n",
    "        profiles = extract_profiles(driver)\n",
    "        save_to_csv(profiles)\n",
    "\n",
    "        driver.find_element(By.CSS_SELECTOR, \"button.artdeco-modal__dismiss\").click()\n",
    "        time.sleep(1)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Erro no elemento {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98f1c20d",
   "metadata": {},
   "source": [
    "Pode acontecer de uma mesma pessoa ter curtido dois posts que coletamos, então abaixo se extrai as entradas duplicadas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b864a2e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('linkedin_profiles.csv')\n",
    "\n",
    "df_unico = df.drop_duplicates(subset='Link')\n",
    "\n",
    "df_unico.to_csv('linkedin_profiles.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "444eda5a",
   "metadata": {},
   "source": [
    "## Coleta de dados de cada um dos perfis\n",
    "\n",
    "Optamos por acessar individualmente as páginas dos perfis coletados e salvar o HTML de cada uma delas na pasta /html_dumps, em vez de extrair diretamente as informações. Essa abordagem nos permite ajustar posteriormente o parsing dos dados, sem depender do acesso contínuo ao LinkedIn — o que reduz o risco de bloqueios.\n",
    "\n",
    "Essa decisão se mostrou essencial, já que, após a visualização do centésimo perfil, o LinkedIn deixou de carregar as páginas normalmente. Para prosseguir além disso, seria necessário criar novas contas e/ou utilizar rotação de IP. Diante disso, optamos por restringir, ao menos inicialmente, a análise aos 100 primeiros perfis coletados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37418f0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def save_linkedin_profile_html(driver, profile_url: str, output_dir: str = \"html_dumps\", timeout: int = 10) -> Optional[str]:\n",
    "\n",
    "    try:\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        driver.get(profile_url)\n",
    "\n",
    "        WebDriverWait(driver, timeout).until(\n",
    "            EC.presence_of_element_located((By.CSS_SELECTOR, \"main\"))\n",
    "        )\n",
    "\n",
    "        html = driver.page_source\n",
    "\n",
    "        profile_id = profile_url.rstrip('/').split('/')[-1]\n",
    "        filename = f\"{profile_id}.html\"\n",
    "        filepath = os.path.join(output_dir, filename)\n",
    "\n",
    "        with open(filepath, \"w\", encoding=\"utf-8\") as f:\n",
    "            f.write(html)\n",
    "\n",
    "        print(f\"[✔] HTML salvo para {profile_url} em: {filepath}\")\n",
    "        return filepath\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[✘] Falha ao salvar HTML de {profile_url}: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066cd51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('linkedin_profiles.csv')\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    html_path = save_linkedin_profile_html(driver, row['Link'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db38d375",
   "metadata": {},
   "source": [
    "## Classificação e Análise\n",
    "\n",
    "A partir desse ponto, o processo seguiu da seguinte forma: utilizamos três funções principais para extrair as informações relevantes de cada perfil. A primeira, `extrair_infos_gerais`, coleta dados como nome, cargo atual, localização e o link do perfil. Em seguida, a função `extrair_experiencias` reúne todas as experiências profissionais da pessoa, incluindo o cargo ocupado, o período de atuação e o link da empresa correspondente. Por fim, a função `extrair_educacao` obtém todas as formações acadêmicas listadas.\n",
    "\n",
    "A partir das informações extraídas, estruturamos um dicionário por perfil e, em seguida, transformamos esses dados para o formato padronizado do dataset State of The Data. Os atributos extraídos, junto com o grau de confiança em cada um deles, são os seguintes:\n",
    "\n",
    "* **UF onde a pessoa mora**: confiança **total**. Essa informação é declarada explicitamente no perfil, normalmente na seção de localização, e foi extraída de forma direta.\n",
    "\n",
    "* **Tempo de experiência em dados**: confiança **moderada (\\~70%)**. Dependemos das experiências listadas no perfil, que podem estar incompletas ou omitidas, além de possíveis diferenças na forma como o usuário entende e relata sua atuação na área.\n",
    "\n",
    "* **Tempo prévio de experiência em TI**: confiança **moderada (\\~70%)**, pelos mesmos motivos acima. Há chance de a pessoa não ter listado todo o histórico profissional ou de interpretar de forma distinta o que conta como experiência em TI.\n",
    "\n",
    "* **Nível mais alto de escolaridade até novembro de 2024**: confiança **quase total**. É altamente provável que o LinkedIn reflita corretamente a formação mais avançada do usuário, e conseguimos capturar essa informação com precisão.\n",
    "\n",
    "* **Área principal de formação**: confiança **quase total**. A área de formação costuma estar bem especificada no perfil, e foi extraída diretamente da seção de educação.\n",
    "\n",
    "* **Cargo atual em novembro de 2024**: confiança **quase total**. Utilizamos o cargo listado mais recentemente até a data de corte e o normalizamos conforme as categorias do dataset, o que nos dá alta confiança na correspondência.\n",
    "\n",
    "* **Link da empresa em novembro de 2024**: **dado auxiliar, ainda não um atributo final**. Embora tenhamos coletado o link da empresa referente ao cargo atual, ele será utilizado posteriormente para enriquecer os dados com o **setor da empresa** e o **número de funcionários**, atributos que ainda serão incluídos.\n",
    "\n",
    "Essas informações foram passadas para uma LLM, junto com os possíveis valores esperados para cada campo, a fim de realizar a classificação e gerar uma versão estruturada para cada perfil. Por fim, salvamos o resultado em um arquivo CSV chamado `linkedin_profiles_data.csv`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5b6f96e",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "openai.api_key = \"\"\n",
    "DIRETORIO_HTML = \"html_dumps\"\n",
    "ARQUIVO_SAIDA = \"linkedin_profiles_data.csv\"\n",
    "\n",
    "def limpar_json(raw):\n",
    "    if raw.startswith(\"```\"):\n",
    "        raw = raw.strip().strip(\"`\")\n",
    "        inicio = raw.find(\"{\")\n",
    "        fim = raw.rfind(\"}\")\n",
    "        if inicio != -1 and fim != -1:\n",
    "            raw = raw[inicio:fim+1]\n",
    "    return raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cffebc",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extrair_infos_gerais(soup):\n",
    "    nome_tag = soup.find(\"h1\", class_=\"aYftUZQtwiflyYvLNDOYHdctvSKFncppEBZDg inline t-24 v-align-middle break-words\")\n",
    "    nome = nome_tag.get_text(strip=True) if nome_tag else \"N/A\"\n",
    "\n",
    "    cargo_tag = soup.find(\"div\", class_=\"text-body-medium break-words\")\n",
    "    cargo = cargo_tag.get_text(strip=True) if cargo_tag else \"N/A\"\n",
    "\n",
    "    local_tag = soup.find(\"span\", class_=\"text-body-small inline t-black--light break-words\")\n",
    "    localizacao = local_tag.get_text(strip=True) if local_tag else \"N/A\"\n",
    "\n",
    "    link_tag = soup.find(\"a\", class_=\"IqLKNJXIVaTxfTzhuwyMDMsOBGXIClUI\")\n",
    "    href = link_tag[\"href\"] if link_tag else None\n",
    "    if href and href.startswith(\"/in/\"):\n",
    "        base_href = href.split(\"/overlay\")[0]\n",
    "        link_perfil = f\"https://www.linkedin.com{base_href}/\"\n",
    "    else:\n",
    "        link_perfil = \"N/A\"\n",
    "\n",
    "    return nome, cargo, localizacao, link_perfil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1305c7cd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def encontrar_secao_experiencia(soup):\n",
    "    for section in soup.find_all(\"section\"):\n",
    "        h2 = section.find(\"h2\", class_=\"pvs-header__title\")\n",
    "        if h2 and \"Experiência\" in h2.get_text():\n",
    "            return section\n",
    "    return None\n",
    "\n",
    "def extrair_experiencias(section):\n",
    "    experiencias = []\n",
    "    blocos_empresa = section.find_all(\"div\", attrs={\"data-view-name\": \"profile-component-entity\"}, recursive=True)\n",
    "\n",
    "    for bloco in blocos_empresa:\n",
    "        is_empresa = bloco.find(\"div\", class_=lambda c: c and \"flex-column\" in c)\n",
    "        if not is_empresa:\n",
    "            continue\n",
    "\n",
    "        link_empresa_tag = bloco.find(\"a\", href=True)\n",
    "        link_empresa = f\"https://www.linkedin.com{link_empresa_tag['href']}\" if link_empresa_tag else \"N/A\"\n",
    "\n",
    "        subcargos = bloco.find_all(\"div\", attrs={\"data-view-name\": \"profile-component-entity\"}, recursive=False)\n",
    "        if len(subcargos) > 1:\n",
    "            for sub in subcargos:\n",
    "                cargo = sub.find(\"div\", class_=lambda c: c and \"t-bold\" in c)\n",
    "                cargo_span = cargo.find(\"span\", attrs={\"aria-hidden\": \"true\"}) if cargo else None\n",
    "                nome_cargo = cargo_span.get_text(strip=True) if cargo_span else \"N/A\"\n",
    "\n",
    "                periodo_tag = sub.find(\"span\", class_=\"pvs-entity__caption-wrapper\")\n",
    "                periodo = periodo_tag.get_text(strip=True) if periodo_tag else \"N/A\"\n",
    "\n",
    "                experiencias.append({\n",
    "                    \"cargo\": nome_cargo,\n",
    "                    \"periodo\": periodo,\n",
    "                    \"link_empresa\": link_empresa\n",
    "                })\n",
    "        else:\n",
    "            cargo_tag = bloco.find(\"div\", class_=lambda c: c and \"t-bold\" in c)\n",
    "            cargo_span = cargo_tag.find(\"span\", attrs={\"aria-hidden\": \"true\"}) if cargo_tag else None\n",
    "            nome_cargo = cargo_span.get_text(strip=True) if cargo_span else \"N/A\"\n",
    "\n",
    "            periodo_tag = bloco.find(\"span\", class_=\"pvs-entity__caption-wrapper\")\n",
    "            periodo = periodo_tag.get_text(strip=True) if periodo_tag else \"N/A\"\n",
    "\n",
    "            experiencias.append({\n",
    "                \"cargo\": nome_cargo,\n",
    "                \"periodo\": periodo,\n",
    "                \"link_empresa\": link_empresa\n",
    "            })\n",
    "\n",
    "    return experiencias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06257dae",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def extrair_educacao(soup):\n",
    "    educacoes = []\n",
    "    for section in soup.find_all(\"section\"):\n",
    "        h2 = section.find(\"h2\", class_=\"pvs-header__title\")\n",
    "        if h2 and \"Formação acadêmica\" in h2.get_text(strip=True):\n",
    "            ul = section.find(\"ul\")\n",
    "            if not ul:\n",
    "                continue\n",
    "            for li in ul.find_all(\"li\", recursive=False):\n",
    "                instituicao_span = li.find(\"div\", class_=\"t-bold\")\n",
    "                if instituicao_span:\n",
    "                    instituicao_span = instituicao_span.find(\"span\", attrs={\"aria-hidden\": \"true\"})\n",
    "                curso_span = li.find(\"span\", class_=\"t-14 t-normal\")\n",
    "                if curso_span:\n",
    "                    curso_span = curso_span.find(\"span\", attrs={\"aria-hidden\": \"true\"})\n",
    "                periodo_span = li.find(\"span\", class_=\"t-14 t-normal t-black--light\")\n",
    "                if periodo_span:\n",
    "                    periodo_span = periodo_span.find(\"span\", attrs={\"aria-hidden\": \"true\"})\n",
    "\n",
    "                educacoes.append({\n",
    "                    \"instituicao\": instituicao_span.get_text(strip=True) if instituicao_span else None,\n",
    "                    \"curso\": curso_span.get_text(strip=True) if curso_span else None,\n",
    "                    \"periodo\": periodo_span.get_text(strip=True) if periodo_span else None\n",
    "                })\n",
    "            break\n",
    "    return educacoes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7599cde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classificar_perfil_com_llm(dados_perfil: dict) -> dict:\n",
    "    prompt = f\"\"\"\n",
    "A seguir está um dicionário com informações extraídas de um perfil do LinkedIn. Considere **novembro de 2024** como a data de referência para responder às perguntas abaixo. Algumas experiências podem conter ruído (por exemplo, apenas o nome da empresa ou informações incompletas), então desconsidere qualquer entrada que não pareça de fato um cargo exercido.\n",
    "\n",
    "Classifique com base nas seguintes opções:\n",
    "\n",
    "1. **UF onde mora** (`uf_onde_mora`): sigla do estado.\n",
    "['RS', 'SC', 'SP', 'DF', 'MA', 'BA', 'MG', 'PR', 'MT', 'GO', 'AL', 'PB', 'PE', 'RJ', 'ES', 'AP', 'CE', 'TO', 'PI', 'MS', 'RN', 'AM', 'RO', 'SE', 'PA']\n",
    "\n",
    "2. **Tempo de experiência na área de dados** (`tempo_de_experiencia_em_dados`):\n",
    "['Menos de 1 ano', 'de 1 a 2 anos', 'de 3 a 4 anos', 'de 5 a 6 anos', 'de 7 a 10 anos', 'Mais de 10 anos', 'Não tenho experiência na área de dados']\n",
    "\n",
    "3. **Tempo de experiência em TI antes de atuar com dados** (`tempo_de_experiencia_em_ti`):\n",
    "['Menos de 1 ano', 'de 1 a 2 anos', 'de 3 a 4 anos', 'de 5 a 6 anos', 'de 7 a 10 anos', 'Mais de 10 anos', 'Não tive experiência na área de TI/Engenharia de Software antes de começar a trabalhar na área de dados']\n",
    "\n",
    "4. **Nível de ensino mais alto alcançado até novembro de 2024** (`nivel_de_ensino`):\n",
    "['Estudante de Graduação', 'Graduação/Bacharelado', 'Pós-graduação', 'Mestrado', 'Doutorado ou Phd', 'Não tenho graduação formal', 'Prefiro não informar']\n",
    "\n",
    "5. **Área de formação principal** (`area_de_formação`):\n",
    "['Computação / Engenharia de Software / Sistemas de Informação/ TI', 'Economia/ Administração / Contabilidade / Finanças/ Negócios', 'Estatística/ Matemática / Matemática Computacional/ Ciências Atuariais', 'Outra opção', 'Outras Engenharias (não incluir engenharia de software ou TI)', 'Ciências Biológicas/ Farmácia/ Medicina/ Área da Saúde', 'Marketing / Publicidade / Comunicação / Jornalismo / Ciências Sociais', 'Química / Física']\n",
    "\n",
    "7. **Cargo atual da pessoa em novembro de 2024** (`cargo_atual`):\n",
    "['Analista de Dados/Data Analyst', 'Analista de BI/BI Analyst', 'Cientista de Dados/Data Scientist', 'Engenheiro de Dados/Data Engineer/Data Architect', 'Engenheiro de Machine Learning/ML Engineer/AI Engineer', 'Analytics Engineer', 'Data Product Manager/ Product Manager (PM/APM/DPM/GPM/PO)', 'Analista de Negócios/Business Analyst', 'Analista de Suporte/Analista Técnico', 'Professor/Pesquisador', 'Desenvolvedor/ Engenheiro de Software/ Analista de Sistemas', 'Arquiteto de Dados/Data Architect', 'Estatístico', 'Outra Opção', 'Outras Engenharias (não inclui dev)']\n",
    "\n",
    "8. **Link da empresa onde a pessoa estava em novembro de 2024** (`link_da_empresa_em_novembro_2024`): se conseguir identificar\n",
    "\n",
    "Retorne no seguinte formato JSON:\n",
    "{{\n",
    "    \"uf_onde_mora\": \"\",\n",
    "    \"tempo_de_experiencia_em_dados\": \"\",\n",
    "    \"tempo_de_experiencia_em_ti\": \"\",\n",
    "    \"nivel_de_ensino\": \"\",\n",
    "    \"area_de_formação\": \"\",\n",
    "    \"cargo_atual\": \"\",\n",
    "    \"link_da_empresa_em_novembro_2024\": \"\"\n",
    "}}\n",
    "\n",
    "Dicionário com dados do perfil:\n",
    "{json.dumps(dados_perfil, ensure_ascii=False, indent=2)}\n",
    "\"\"\".strip()\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        temperature=0,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"Você é um classificador de perfis de LinkedIn.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    resposta_texto = response.choices[0].message.content.strip()\n",
    "    try:\n",
    "        resposta_limpa = limpar_json(resposta_texto)\n",
    "        return json.loads(resposta_limpa)\n",
    "    except Exception as e:\n",
    "        print(\"Erro ao interpretar resposta da LLM:\", e)\n",
    "        print(\"Resposta bruta:\", resposta_texto)\n",
    "        return {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a28b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "\n",
    "# for filename in os.listdir(DIRETORIO_HTML):\n",
    "#     if not filename.endswith(\".html\"):\n",
    "#         continue\n",
    "\n",
    "filepath = os.path.join(DIRETORIO_HTML, 'ACoAAAH4yPQBCgngA7AVez1BQFxvXL67TcUwj4E.html')\n",
    "with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "    soup = BeautifulSoup(f, \"html.parser\")\n",
    "\n",
    "nome, cargo, localizacao, link_perfil = extrair_infos_gerais(soup)\n",
    "secao_experiencia = encontrar_secao_experiencia(soup)\n",
    "experiencias = extrair_experiencias(secao_experiencia) if secao_experiencia else []\n",
    "educacoes = extrair_educacao(soup)\n",
    "\n",
    "dados_perfil = {\n",
    "    \"nome\": nome,\n",
    "    \"cargo_atual\": cargo,\n",
    "    \"localizacao\": localizacao,\n",
    "    \"experiencias\": experiencias,\n",
    "    \"formacoes\": educacoes,\n",
    "    \"link_perfil\": link_perfil\n",
    "}\n",
    "\n",
    "resumo = classificar_perfil_com_llm(dados_perfil)\n",
    "data.append({\n",
    "    \"nome\": nome,\n",
    "    \"localizacao_bruta\": localizacao,\n",
    "    \"link_perfil\": link_perfil,\n",
    "    \"cargo_bruto\": cargo,\n",
    "    \"uf_onde_mora\": resumo.get(\"uf_onde_mora\", \"\"),\n",
    "    \"tempo_de_experiencia_em_dados\": resumo.get(\"tempo_de_experiencia_em_dados\", \"\"),\n",
    "    \"tempo_de_experiencia_em_ti\": resumo.get(\"tempo_de_experiencia_em_ti\", \"\"),\n",
    "    \"nivel_de_ensino\": resumo.get(\"nivel_de_ensino\", \"\"),\n",
    "    \"area_de_formação\": resumo.get(\"area_de_formação\", \"\"),\n",
    "    \"cargo_atual\": resumo.get(\"cargo_atual\", \"\"),\n",
    "    \"link_da_empresa_em_novembro_2024\": resumo.get(\"link_da_empresa_em_novembro_2024\", \"\")\n",
    "})\n",
    "\n",
    "print({\n",
    "    \"nome\": nome,\n",
    "    \"localizacao_bruta\": localizacao,\n",
    "    \"link_perfil\": link_perfil,\n",
    "    \"cargo_bruto\": cargo,\n",
    "    \"uf_onde_mora\": resumo.get(\"uf_onde_mora\", \"\"),\n",
    "    \"tempo_de_experiencia_em_dados\": resumo.get(\"tempo_de_experiencia_em_dados\", \"\"),\n",
    "    \"tempo_de_experiencia_em_ti\": resumo.get(\"tempo_de_experiencia_em_ti\", \"\"),\n",
    "    \"nivel_de_ensino\": resumo.get(\"nivel_de_ensino\", \"\"),\n",
    "    \"area_de_formação\": resumo.get(\"area_de_formação\", \"\"),\n",
    "    \"cargo_atual\": resumo.get(\"cargo_atual\", \"\"),\n",
    "    \"link_da_empresa_em_novembro_2024\": resumo.get(\"link_da_empresa_em_novembro_2024\", \"\")\n",
    "})\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv(ARQUIVO_SAIDA, index=False, encoding=\"utf-8\")\n",
    "print(f\"✅ Arquivo '{ARQUIVO_SAIDA}' criado com sucesso.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e32792",
   "metadata": {
    "lines_to_next_cell": 3
   },
   "source": [
    "### Próximos Passos — Enriquecimento com dados das empresas\n",
    "\n",
    "Como sugestão de melhoria, o próximo passo seria enriquecer o dataset `linkedin_profiles_data.csv` com informações das empresas onde os profissionais estavam atuando em **novembro de 2024**.\n",
    "\n",
    "A última coluna do dataset já contém o **link para o perfil da empresa** no LinkedIn. A partir desses links, o ideal fazer um parser para acessar a página de cada empresa e coletar:\n",
    "\n",
    "* **Setor da empresa** (ex: Fintech, Educação, Saúde, etc.)\n",
    "* **Número de funcionários** (em faixas como \"11–50\", \"201–500\", etc.)\n",
    "\n",
    "**Observação importante**:\n",
    "\n",
    "O LinkedIn geralmente **não exibe o setor da empresa de forma estruturada**. No entanto, a **seção \"Sobre\" (About)** costuma conter uma descrição textual rica que pode ser utilizada como fonte. A ideia seria:\n",
    "\n",
    "1. Extrair o texto da seção \"Sobre\" de cada empresa.\n",
    "2. Passar esse texto para uma **LLM**, junto com os **valores possíveis de setores** (obtidos a partir dos `unique()` do dataset original), os quais são os seguintes: \n",
    "\n",
    "['Marketing' 'Finanças ou Bancos' 'Indústria'\n",
    " 'Tecnologia/Fábrica de Software' nan 'Telecomunicação' 'Outra Opção'\n",
    " 'Setor Público' 'Área da Saúde' 'Área de Consultoria' 'Varejo'\n",
    " 'Entretenimento ou Esportes' 'Setor Automotivo'\n",
    " 'Setor Imobiliário/ Construção Civil' 'Educação' 'Internet/Ecommerce'\n",
    " \"Filantropia/ONG's\" 'Seguros ou Previdência' 'Agronegócios'\n",
    " 'Setor de Energia' 'Setor Alimentício' 'Setor Farmaceutico']\n",
    "\n",
    "3. Solicitar à LLM que classifique a empresa no setor mais adequado com base no conteúdo do texto.\n",
    "\n",
    "O mesmo pode ser feito para o **número de funcionários**. Embora o Linkedin exiba essa informação, o **formato pode variar** e pode não estar padronizado conforme as categorias do nosso dataset. Por isso, a ideia também é:\n",
    "\n",
    "1. Extrair a estimativa bruta do número de funcionários (ex: \"201-500 employees\").\n",
    "2. Passar esse valor para a **LLM**, junto com as faixas esperadas no dataset, as quais são:\n",
    "\n",
    "['de 101 a 500' 'Acima de 3.000' 'de 501 a 1.000' 'de 1.001 a 3.000' nan\n",
    " 'de 6 a 10' 'de 11 a 50' 'de 1 a 5' 'de 51 a 100']"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "formats": "py,ipynb",
   "main_language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
